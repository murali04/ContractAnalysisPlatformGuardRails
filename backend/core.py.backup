import os
import shutil
import uuid
import logging
import json
import fitz  # PyMuPDF
import pandas as pd
import numpy as np
from datetime import datetime
from dotenv import load_dotenv
from keybert import KeyBERT
from openai import OpenAI
from langdetect import detect
from googletrans import Translator
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.docstore.document import Document
from sklearn.metrics.pairwise import cosine_similarity

# Setup logging
logging.basicConfig(level=logging.INFO)
load_dotenv()

# Initialize globals
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
embedder = OpenAIEmbeddings(model="text-embedding-3-small")
kw_model = KeyBERT()
translator = Translator()

import spacy
try:
    nlp = spacy.load("en_core_web_sm")
except OSError:
    # Download if not present (though usually better to do in docker/setup)
    from spacy.cli import download
    download("en_core_web_sm")
    nlp = spacy.load("en_core_web_sm")

USER_DIR = "user_memory"
os.makedirs(USER_DIR, exist_ok=True)

def detect_language(text):
    try:
        lang_code = detect(text)
        flags = {
            "en": "üá¨üáß", "es": "üá™üá∏", "fr": "üá´üá∑", "de": "üá©üá™",
            "it": "üáÆüáπ", "pt": "üáµüáπ", "hi": "üáÆüá≥", "unknown": "üåê"
        }
        return flags.get(lang_code, "üåê")
    except Exception:
        return "üåê"

def translate_to_english(text):
    try:
        lang_code = detect(text)
        if lang_code != "en" and lang_code != "unknown":
            try:
                translated = translator.translate(text, src=lang_code, dest="en")
                return translated.text
            except Exception:
                # fallback to OpenAI translate if googletrans fails
                resp = client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[
                        {"role": "system", "content": "Translate the following text to English precisely."},
                        {"role": "user", "content": text}
                    ],
                    temperature=0
                )
                return resp.choices[0].message.content.strip()
        return text
    except Exception:
        return text

def get_user_vector_path(session_id):
    run_id = str(uuid.uuid4())[:8]
    return os.path.join(USER_DIR, f"faiss_{session_id}_{run_id}")

def extract_text_from_pdf(file_bytes):
    pdf = fitz.open(stream=file_bytes, filetype="pdf")
    records = []
    for page_num, page in enumerate(pdf, start=1):
        lines = page.get_text("text").split("\n")
        for line_num, line in enumerate(lines, start=1):
            if line.strip():
                records.append({"page": page_num, "line": line_num, "text": line.strip()})
    pdf.close()
    return records

def extract_text_from_docx(file_bytes):
    from docx import Document as DocxDocument
    import io
    doc = DocxDocument(io.BytesIO(file_bytes))
    return [{"page": 1, "line": i + 1, "text": p.text.strip()} for i, p in enumerate(doc.paragraphs) if p.text.strip()]

def extract_text_from_excel(file_bytes):
    import io
    df = pd.read_excel(io.BytesIO(file_bytes)).dropna(how="all")
    df.columns = [str(c).strip() for c in df.columns]
    lines = [" | ".join(map(str, row)) for _, row in df.iterrows()]
    return [{"page": 1, "line": i + 1, "text": t} for i, t in enumerate(lines)]

def extract_text_from_txt(file_bytes):
    text = file_bytes.decode("utf-8")
    return [{"page": 1, "line": i+1, "text": l.strip()} for i, l in enumerate(text.split("\n")) if l.strip()]

def chunk_text(records):
    # CRITICAL FIX: Group text by page first to avoid fragmenting into single lines
    pages = {}
    for rec in records:
        p = rec.get("page", 1)
        if p not in pages:
            pages[p] = []
        pages[p].append(rec["text"])
        
    splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=300)
    docs = []
    
    for page_num, lines in pages.items():
        # Reconstruct page text
        page_text = "\n".join(lines)
        
        # Split into meaningful chunks
        chunks = splitter.split_text(page_text)
        for idx, chunk in enumerate(chunks):
            docs.append(Document(
                page_content=chunk, 
                metadata={"page": page_num, "line": "n/a", "chunk_id": idx}
            ))
            
    return docs

def build_vector_store(docs, path):
    if os.path.exists(path):
        shutil.rmtree(path)
    vs = FAISS.from_documents(docs, embedder)
    vs.save_local(path)
    return vs

def generate_dynamic_keywords(obligations):
    keywords_dict = {}
    for ob in obligations:
        try:
            # Dynamic LLM-based keyword generation for generic applicability
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "You are a legal search expert. Return a JSON object with a key 'keywords' containing 5-8 search terms for the given obligation. \n\nCRITICAL: You must predict the **exact words** a vendor would use to **limit** or **avoid** this obligation.\n- Example: If obligation is 'fix', search for 'refund', 'credit', 'obsolescence'.\n- Example: If obligation is 'indemnify', search for 'defend', 'hold harmless', 'control of defense'.\n- Example: If obligation is 'unlimited', search for 'cap', 'aggregate liability', 'fees paid'.\n\nFocus on specific nouns and verbs found in the contract text."},
                    {"role": "user", "content": ob}
                ],
                temperature=0.2,
                response_format={"type": "json_object"}
            )
            res_text = response.choices[0].message.content
            parsed = json.loads(res_text)
            keywords = parsed.get("keywords", [])
            
            # HYBRID STRATEGY: Combine LLM's context-aware keywords with a "Universal Safety Net"
            # These terms are universally relevant for finding limitations/escapes in ANY contract.
            universal_danger_words = ["refund", "reimburse", "terminate", "cap", "limit", "sole discretion", "exclusive remedy", "unless", "except", "notwithstanding", "subject to", "provided that"]
            keywords.extend(universal_danger_words)
            
            keywords_dict[ob] = list(set(keywords))
        except Exception as e:
            logging.error(f"Keyword generation failed for '{ob}': {e}")
            # Fallback: simple noun chunks + danger words
            try:
                doc = nlp(ob)
                base_kws = [chunk.text.lower() for chunk in doc.noun_chunks]
                base_kws.extend(["refund", "reimburse", "terminate", "cap", "limit"])
                keywords_dict[ob] = base_kws
            except:
                keywords_dict[ob] = ob.split()[:5] + ["refund", "reimburse", "terminate"]
                
    return keywords_dict

def query_rag(vs, obligation, auto_keywords, top_k=10):
    retriever = vs.as_retriever(search_type="similarity", search_kwargs={"k": top_k})
    docs = retriever.get_relevant_documents(obligation)
    
    if not docs:
        return {
            "obligation": obligation, "is_present": "No", "reason": "No relevant clauses retrieved.",
            "similarity_score": 0.0, "keyword_hits": [], "confidence": 0.0,
            "page": None, "line": None, "supporting_clauses": []
        }
    
    ob_emb = embedder.embed_query(obligation)
    doc_embs = [embedder.embed_query(d.page_content) for d in docs]
    sims = cosine_similarity([ob_emb], doc_embs)[0]
    best_idx = int(np.argmax(sims))
    best_doc = docs[best_idx]
    best_score = float(sims[best_idx])
    
    obligation_keywords = auto_keywords.get(obligation, [])
    keyword_hits = [kw for kw in obligation_keywords if kw.lower() in best_doc.page_content.lower()]
    keyword_ratio = len(keyword_hits) / max(1, len(obligation_keywords))
    
    # Cosine Similarity Only for Confidence
    confidence = round(best_score * 100, 1)
    
    # Generic Principle-Based Prompt
    prompt = f"""
You are a contract compliance analyst. Analyze whether the contract clause satisfies the obligation.

CRITICAL: Focus on LEGAL EFFECT and COMMERCIAL OUTCOME, not exact wording.

Analysis Framework:
1. **Identify the Obligation's Purpose**: What commercial outcome or risk allocation does the obligation seek?
2. **Analyze the Clause's Effect**: Does the clause achieve the same outcome or provide equivalent protection?
3. **Apply Materiality Test**: Is there a material difference that significantly changes the business value or risk?

Return "Yes" if:
- The clause achieves the same commercial/legal outcome as the obligation
- Any differences are immaterial or standard legal practice
- Alternative methods to achieve the result are acceptable

Return "No" ONLY if:
- The clause negates the obligation's core purpose
- The clause introduces a material escape that significantly shifts risk
- The clause narrows the obligation in a way that excludes common scenarios
- The clause adds conditions that re-impose obligations the original sought to exclude

Key Principles:

**Alternative Remedies**: Multiple paths to the same outcome = compliant
- Example: "modify OR secure licenses" both prevent infringement and ensure continued use ‚Üí YES
- Example: "fix OR refund and terminate" have different outcomes (continued use vs. termination) ‚Üí NO
- CRITICAL: "Secure licenses", "procure rights", "obtain permissions" mean CONTINUED USE (not termination) ‚Üí These are acceptable alternatives to modification
- CRITICAL: "Reimburse", "refund", "credit" are TERMINATION options (customer gets money back and stops using the product)
- If the obligation requires "continued use", "replace", or "secure rights", and the clause offers "reimburse/refund/credit" as an alternative, this is a material deviation ‚Üí NO
- Example: Obligation requires "secure continued use OR replace". Clause offers "secure rights OR substitute OR reimburse". The "reimburse" option allows termination instead of continued use ‚Üí NO

**Discretion**: Discretion about HOW to achieve a result ‚â† discretion about WHETHER to achieve it
- Example: "Vendor chooses remedy method (fix, license, replace)" = discretion on HOW ‚Üí YES
- Example: "Vendor may provide support if deemed reasonable" = discretion on WHETHER ‚Üí NO
- CRITICAL: If the clause says "at vendor's sole discretion" but provides multiple remedy options (e.g., "modify OR secure licenses"), the vendor MUST act - they just choose which method ‚Üí This is discretion on HOW, not WHETHER ‚Üí YES
- CRITICAL: Look for the COMMITMENT to achieve the result. If the clause commits to achieving the outcome (e.g., "ensure continued use", "remedy infringement"), the discretion is only about the method
- Example: "Vendor may, at its sole discretion, implement modifications OR secure licenses to ensure continued use" ‚Üí Vendor MUST ensure continued use (commitment), discretion is only on method (modify vs. license) ‚Üí YES

**Standard Exceptions**: Legal/regulatory carve-outs are acceptable for POSITIVE obligations
- Example: "keep confidential UNLESS required by law" = standard exception ‚Üí YES
- Example: "not liable for damages EXCEPT gross negligence" = standard exception ‚Üí YES
- Example: "keep confidential UNLESS needed for business purposes" = broad exception ‚Üí NO

**Negative Obligations (Exclusions)**: If the obligation says vendor is "NOT liable" or "does not have to" do something, analyze carefully:
- The PURPOSE is to EXCLUDE vendor liability in specific scenarios
- If the clause adds "unless" or "except" conditions, ask: Do these conditions RE-IMPOSE the liability the obligation sought to exclude?
- If YES (the exceptions re-impose liability), return "No" - the exceptions negate the exclusion
- CONCRETE EXAMPLE: 
  - Obligation: "Vendor does not have to indemnify Bank if infringement is solely from Bank's unauthorized modification."
  - Clause: "Vendor has no obligation UNLESS: (1) modification was required by license, (2) modification was required by documentation, (3) modification was mutually agreed."
  - Analysis: These "unless" exceptions RE-IMPOSE indemnification liability for common scenarios (modifications required by docs, etc.), negating the exclusion ‚Üí "No"
- The key question: Do the exceptions make the vendor liable again for scenarios the obligation said they should NOT be liable for? If yes ‚Üí "No"

**Scope**: Broad exceptions that negate "all" or "any" = material conflict
- Example: "return ALL info EXCEPT for business, legal, disaster recovery" = negates "ALL" ‚Üí NO
- Example: "indemnify EXCEPT customer modifications" = standard carve-out ‚Üí YES

Return JSON:
{{
  "is_present": "Yes" or "No",
  "reason": "Explain the legal effect and whether it matches the obligation's purpose",
  "suggestion": "If 'No', suggest specific language to achieve compliance. If 'Yes', return null."
}}

Obligation:
{obligation}

Relevant Clauses:
{chr(10).join([d.page_content for d in docs])}
"""
    print("DEBUG: Generated Prompt:\n", prompt) # Debugging line
    llm_status, llm_reason = "No", ""
    try:
        # Chain-of-Thought Analysis with GPT-4o
        cot_prompt = f"""
{prompt}

IMPORTANT: Before providing your final JSON answer, you MUST think step-by-step:

Step 1: What is the EXACT purpose of the obligation? (What outcome does it seek?)
Step 2: What does the clause ACTUALLY say? (Summarize the legal effect)
Step 3: Do they match? (Does the clause achieve the same outcome?)
Step 4: Are there any material conflicts? (Does the clause negate or weaken the obligation?)
Step 5: CRITICAL CHECK - If the obligation requires "continued use", "replace", or "secure rights", does the clause offer "reimburse", "refund", or "credit" as an option? If YES, this is a TERMINATION option that conflicts with continued use ‚Üí Answer must be "No"
Step 6: CRITICAL CHECK - If the clause says "at vendor's sole discretion", does it provide multiple remedy options (e.g., "modify OR secure licenses")? If YES, this is discretion on HOW (method), not WHETHER (outcome) ‚Üí The vendor MUST act, they just choose the method ‚Üí Answer should be "Yes" if the methods achieve the same outcome

After completing these steps, provide your final JSON answer.
"""
        
        resp = client.chat.completions.create(
            model="gpt-4o-mini",  # Switched back to gpt-4o-mini as requested
            messages=[
                {"role": "system", "content": "You are a meticulous contract compliance expert. Think step-by-step before answering."},
                {"role": "user", "content": cot_prompt}
            ],
            temperature=0.0,  # Deterministic reasoning
            max_tokens=800
        )
        res_text = resp.choices[0].message.content.strip()
        print("DEBUG: LLM Response:\n", res_text)
        
        # Extract JSON from response
        if "```json" in res_text:
            json_text = res_text.split("```json")[1].split("```")[0]
        elif "```" in res_text:
            json_text = res_text.split("```")[1]
        else:
            json_text = res_text
            
        parsed = json.loads(json_text)
        llm_status = parsed.get("is_present", "No").strip()
        
        # Normalize to Title Case (Yes/No)
        if llm_status.lower() == "yes":
            llm_status = "Yes"
        elif llm_status.lower() == "no":
            llm_status = "No"
        
        # Enforce strict Yes/No
        if llm_status not in ["Yes", "No"]:
            llm_status = "No"
            
        llm_reason = parsed.get("reason", "").strip()
        llm_suggestion = parsed.get("suggestion", None)
        
        # Fix suggestion logic: null/None for Yes, actual suggestion for No
        if llm_status == "Yes":
            llm_suggestion = None
        elif not llm_suggestion or llm_suggestion == "null":
            llm_suggestion = "Consider adding explicit language to address this obligation."
    except Exception:
        llm_reason = "Reason could not be parsed from model response."
        llm_suggestion = "Could not generate suggestion due to error."
    
    # Final Status based on LLM (Semantic Analysis)
    final_status = llm_status
    
    # Removed strict similarity threshold to allow LLM reasoning (semantic match) to prevail
    # even if cosine similarity is low due to vocabulary differences.

    supporting_clauses = [f"[Page {d.metadata.get('page')} Line {d.metadata.get('line')}] {d.page_content[:250].strip()}" for d in docs]
    
    return {
        "obligation": obligation, "is_present": final_status, "reason": llm_reason,
        "similarity_score": round(best_score, 3), "keyword_hits": keyword_hits,
        "confidence": confidence, "page": best_doc.metadata.get("page"), "line": best_doc.metadata.get("line"),
        "supporting_clauses": supporting_clauses,
        "suggestion": llm_suggestion
    }

def analyze_contract(obligations_file_bytes, obligations_filename, contract_file_bytes, contract_filename, session_id):
    # 1. Load Obligations
    import io
    if obligations_filename.endswith(".csv"):
        df_ob = pd.read_csv(io.BytesIO(obligations_file_bytes)).dropna(how="all")
    else:
        df_ob = pd.read_excel(io.BytesIO(obligations_file_bytes)).dropna(how="all")
    
    df_ob.columns = [str(c).strip() for c in df_ob.columns]
    
    # Translate obligations
    df_ob["Language"] = df_ob.iloc[:, 0].astype(str).apply(lambda x: detect_language(x))
    df_ob["Obligation_English"] = df_ob.iloc[:, 0].astype(str).apply(lambda x: translate_to_english(x).strip())
    df_ob = df_ob[df_ob["Obligation_English"].str.strip() != ""].reset_index(drop=True)
    obligations = df_ob["Obligation_English"].tolist()
    
    # 2. Extract Contract Text
    if contract_filename.endswith(".pdf"):
        records = extract_text_from_pdf(contract_file_bytes)
    elif contract_filename.endswith(".docx"):
        records = extract_text_from_docx(contract_file_bytes)
    elif contract_filename.endswith(".xlsx"):
        records = extract_text_from_excel(contract_file_bytes)
    else:
        records = extract_text_from_txt(contract_file_bytes)
        
    # Translate contract text
    for rec in records:
        rec["text"] = translate_to_english(rec["text"]).strip()
        
    # 3. Build Vector Store
    docs = chunk_text(records)
    vector_path = get_user_vector_path(session_id)
    vs = build_vector_store(docs, vector_path)
    
    # 4. Generate Keywords
    auto_keywords = generate_dynamic_keywords(obligations)
    
    # 5. Run Analysis
    results = []
    for ob in obligations:
        results.append(query_rag(vs, ob, auto_keywords))
        
    # Cleanup vector store
    # shutil.rmtree(vector_path) # Optional: keep it for caching or delete
    
    # 6. Prepare Full Text for Preview Fallback
    full_text = "\n\n".join([r["text"] for r in records])

    return results, full_text
